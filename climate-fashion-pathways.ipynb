{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mrchristian/climate/blob/master/climate-fashion-pathways.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AXDshvzTef_m"
      },
      "source": [
        "## Package installation: `docanalysis` , `py4ami` and `pygetpapers`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Be74i2ErAmIi"
      },
      "outputs": [],
      "source": [
        "!pip install docanalysis\n",
        "!git clone https://github.com/petermr/semanticClimate.git\n",
        "!pip install -r /content/semanticClimate/keyword_extraction/code/requirement.txt\n",
        "import nltk\n",
        "nltk.download('')\n",
        "!python -m spacy download en_core_web_lg\n",
        "import os\n",
        "!pip install py4ami==0.0.35\n",
        "!pip install pipreqs\n",
        "!pip install pygetpapers\n",
        "!python -m spacy download en_core_web_lg"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AqF_rq89fIBo"
      },
      "source": [
        "## Saving literatures from EPMC for the specific query"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lv7Bkuz8CPZw"
      },
      "outputs": [],
      "source": [
        "!docanalysis --run_pygetpapers -q \"carbon credits\" -k 20 --project_name carboncredits\n",
        "!pygetpapers -q \"carbon credits\" -k 20 -o \"carboncredits\" -c --makehtml -x --save_query"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uzpUHPCef2Gj"
      },
      "source": [
        "## Abbreviation extraction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jvX-63vWMCS9"
      },
      "outputs": [],
      "source": [
        "!docanalysis --project_name carboncredits --extract_abb /content/all_abbreviations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "frTOr_-lflIs"
      },
      "source": [
        "## making section to save the xml output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xgfvZGxOESis"
      },
      "outputs": [],
      "source": [
        "!docanalysis --project_name carboncredits --make_section"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D9fLqQt7f8Gh"
      },
      "source": [
        "## Keyword extraction from the literature"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J7rWnzeAdsTX"
      },
      "outputs": [],
      "source": [
        "!python /content/semanticClimate/keyword_extraction/code/climate_justice_keyword.py \\\n",
        "--html_path /content/carboncredits/eupmc_result.html \\\n",
        "--saving_path /content/ \\\n",
        "--method 'rake'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZfwlxRB8gG5I"
      },
      "source": [
        "## Making WordCloud from the keywords"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "THyLJArMaLDi"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from wordcloud import WordCloud\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Step 1: Read the CSV file into a DataFrame\n",
        "csv_file_path = '/content/Rake_keywords.csv'  # Replace with the path to your CSV file\n",
        "df = pd.read_csv(csv_file_path)\n",
        "\n",
        "# Step 2: Extract the keywords from the 'Keyword' column\n",
        "keywords = df['keyword/phrase'].str.cat(sep=' ')\n",
        "\n",
        "# Step 3: Preprocess the keywords (optional)\n",
        "# For example, you can convert text to lowercase:\n",
        "keywords = keywords.lower()\n",
        "\n",
        "# Step 4: Define a list of stop words to exclude from the word cloud\n",
        "stop_words = ['research','system','future','increase','pregnancy', 'chain']  # Add your stop words here\n",
        "\n",
        "# Step 5: Tokenize the keywords into words\n",
        "words = keywords.split()\n",
        "\n",
        "# Step 6: Filter out stop words\n",
        "filtered_words = [word for word in words if word not in stop_words]\n",
        "\n",
        "# Step 7: Create a space-separated string of the filtered keywords\n",
        "filtered_keywords_text = ' '.join(filtered_words)\n",
        "\n",
        "# Step 8: Generate the word cloud\n",
        "wordcloud = WordCloud(width=800, height=400, background_color='white').generate(filtered_keywords_text)\n",
        "\n",
        "# Step 9: Display the word cloud\n",
        "plt.figure(figsize=(12, 15))\n",
        "plt.imshow(wordcloud, interpolation='bilinear')\n",
        "plt.axis('off')\n",
        "plt.show()\n",
        "\n",
        "# If you want to save the word cloud to a file (e.g., 'wordcloud.png'):\n",
        "# wordcloud.to_file('wordcloud.png')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}